% !TEX encoding = UTF-8
% !TEX TS-program = pdflatex
% !TEX root = ../Tesi.tex
% !TEX spellcheck = it-IT

%************************************************
\chapter{CUDA - Compute Unified Device Architecture}
\label{cap:CUDA}
%************************************************

\section{Introduzione}
Quasi nove anni fa, nel Novembre 2006  la \textbf{NVIDIA Corporation} ha
rilasciato CUDA, una piattaforma (hardware e software insieme) che permettono di
utilizzare linguaggi di programmazione ad alto livello (Ad es. \textbf{C},
\textbf{C++}, \textbf{Java}) per implementare codice parallelo per risolvere
problemi molto complessi a livello computazionale in una maniera efficiente
rispetto alle normali CPU.

\begin{figure}[h] 
\centering 
\includegraphics[width=0.5\columnwidth]{Immagini/cuda_compiler} 
\caption[Compilatore NVCC]{Struttura di Nvidia C Compiler.\\}
\label{fig:cuda_compiler} 
\end{figure}

CUDA è molto utilizzato poiché è un sistema completo e anche molto semplice da
capire ed utilizzare. Sopratutto quest'ultimo particolare è di importante
rilevanza, dato che attualmente le alternative a CUDA, come OpenCL, risultano
essere molto più complesse a livello implementativo e di leggibilità  del
codice.
Come illustrato in figura \ref{fig:cuda_compiler}, NVIDIA fornisce un
compilatore capace di riconoscere le istruzioni CUDAma l'implementazione di un
programma parallelo avviene utilizzando codice sorgente sia per CPU che per GPU.
Il compilatore NVIDIA C (\textit{nvcc}) dunque, identifica il tipo di istruzione
richiamando i compilatori di riferimento gestendo così questa convivenza.
  
\section{Architettura hardware}

Oggi sul mercato delle schede video possiamo trovare innumerevoli tipi di
device, e i computer moderni referibilmente posseggono una scheda video
dedicata. In particolare la \textbf{Nvidia Corporation} ha creato anche diverse
architetture hardware per soddisfare ogni tipo di richiesta. Quelle conosciute
sono le architetture \textbf{Kepler}, \textbf{Fermi} e \textbf{Tesla}.
L'architettura Kepler è quella più utilizzata nei computer in commercio con
scheda grafica NVIDIA.

 In generale, le architetture GPU NVIDIA, sono composte da un array di
 \textit{Streaming Multiprocessors (SMs)}. Lo Streaming Multiprocessors è
 progettato per eseguire centinaia di threads in parallelo e contiene un
 determinato numero di Streaming Processors (SP). Gli Streaming processors sono
 anche chiamati \textit{CUDA cores} e il loro numero dipende dalla capacità  del
 device installato.
 
 \subsection{Compute capability}
Ogni device possiede un \textit{revision number} che possiamo definire come la
\textbf{compute capability} del device, e determina l'insieme di funzionalità 
che possono essere usate nell'implementazione di codice parallelo in CUDA.
La compute capability è definita dal più alto numero di revision number e il
minor numero di revision number. Nel caso in cui devices diversi abbiano il più
alto revision number posseggono la stessa architettura. Il più alto numero di
revision number per le architetture Kepler è 3, per i devices basati su
un'architettura Fermi è 2, mentre per i device con architettura Tesla 1. Il
numero minore di revision number invece, corrisponde al miglioramento del core
dell'architettura che spesso porta a nuove funzionalità  da poter utilizzare
tramite le API fornite appunto da NVIDIA.

\subsection{Architettura Kepler}
L'architettura Kepler è stata progettata e successivamente lanciata nel 2010
insieme all'architettura Fermi. La prima GPU basata sull'architettura Kepler si
chiamava ``GK104" in cui ogni unità  interna è stata progettata ai fini di
avere la miglior performance per watt (perf/watt). Alcuni esperti hanno
affermato che la GK104 Kepler è la GPU più potente per la computazione e il
rendering grafico dei videogames.

Inizialmente la GPU utilizzata per questo lavoro di tesi è stata la NVIDIA
GeForce GT 750M basata anch'essa su un architettura Kepler. Il core in
particolare è il ``GK107" che offre due shader di blocchi, chiamati
\textbf{SMX}, ognuno dei quali ha 192 shaders per un totale di 384 shader cores
con una velocità  di 967 MHz.

\begin{figure}[h] 
\centering 
\includegraphics[width=0.5\columnwidth]{Immagini/gt750m} 
\caption[GT 750M]{La scheda video NVIDIA GT 750-M.\\}
\label{fig:gt750m} 
\end{figure} 

\section{Interfaccia di programmazione}
Un programma CUDA consiste in una o più fasi che sono eseguite sia lato host
(\textbf{CPU}) che lato device (\textbf{GPU}). Le fasi in cui l'ammontare
computazionale non è eccessivo, e dunque non siamo in presenza di
parallelismo dei dati, vengono implementate lato host, mentre le fasi che
richiedono un grosso ammontare di parallelismo dei dati sono implementate lato
device. CUDA consente di creare un unico file sorgente con codice host e device
insieme. Il compilatore NVIDIA C (\textbf{nvcc} fig. \ref{fig:cuda_compiler})
separa le due diverse implementazioni durante il processo di compilazione.

Il linguaggio per scrivere codice sorgente lato device è ANSI C, esteso con
particolari \textit{keywords} per far comprendere al compilatore quali sono le
funzioni con la presenza di parallelismo. Queste funzioni sono chiamate
\textbf{\textit{kernels}}. Per utilizzare nvcc naturalmente dobbiamo essere in
possesso di una GPU NVidia correttamente montata sulla propria macchina, ma se
così non fosse si può emulare su CPU le features di CUDA per poter eseguire i
kernels (MCUDA tool etc.).

Le funzioni kernel generano un determinato di threads eseguiti in parallelo per
raggiungere il data parallelism. Ad esempio per la somma di due matrici può
essere implementata come un kernel dove ogni threads computa un elemento
dell'output. Il massimo del parallelismo si ha quando ad ogni threads è
associata una cella della matrice. Se la dimensione della matrice è 1000 x 1000
servono 1 milione di threads per raggiungere il nostro scopo. Lato CPU per
generare e eseguire lo scheduling di un enorme numero di threads è
particolarmente oneroso, mentre in CUDA c'è un ottimo supporto hardware da
questo punto di vista, dunque il programmatore può sorvolare su questo tipo di
problema.

\begin{figure}[h] 
\centering 
\includegraphics[width=0.5\columnwidth]{Immagini/cuda_program} 
\caption[Esecuzione di un programma CUDA]{Esecuzione di un programma CUDA.\\}
\label{fig:cuda_program} 
\end{figure} 

Una tipica esecuzione di un programma CUDA è mostrata nella Fig.
\ref{fig:cuda_program}.
L'esecuzione viene eseguita a strati, la prima ad essere eseguita è la parte
host (CPU) per poi susseguirsi un insieme di strati che possono comportare anche
il lancio dei kernels nel caso ci siano parti parallelizzate. I threads sono
inglobati all'interno di \textbf{blocchi} che a loro volta sono parte di una
griglia di blocchi chiamata \textbf{grid}. Quando un kernel termina, il
programma continua con l'esecuzione lato host fino a che un nuovo kernel viene
lanciato.

\subsection{I kernel}

Come detto in precedenza, la funzione \textit{kernel} specifica il codice che
deve essere eseguito da tutti i threads lanciati nella fase parallela di un
programma CUDA. Tutti i threads lanciati in parallelo eseguono lo stesso
codice, infatti un programma CUDA non è nient'altro che l'applicazione pratica
del modello Single-Program Multiple-Data (Tassonomia di Flynn \ref{par:flynn}).
Questa tecnica è molto utilizzata nei sistemi paralleli. 

Per poter dichiarare un kernel c'è una specifica keyword di CUDA da utilizzare:
``\textbf{\_\_global\_\_}''. Questa keyword indica che la funzione è un kernel e
questa funzione richiamata dall'host genererà una griglia di threads sul device,
in particolare può solamente essere richiamata lato host (a
meno che non ci sia un ambiente addatto per potere utilizzare il parallelismo dinamico
\ref{par:Parallelismo_dinamico}).
CUDA genera threads suddivisi in blocchi, ed ogni blocco appartiene ad una
griglia. Lo schema è mostrato in figura \ref{fig:grid_block}.

\begin{figure}[h] 
\centering 
\includegraphics[width=0.5\columnwidth]{Immagini/grid_block} 
\caption[Griglie e blocchi cuda]{Esempio generico di griglie e blocchi in un
programma CUDA.\\}
\label{fig:grid_block}
\end{figure} 

In realtà, la dimensione della griglia e dei blocchi la decide il programmatore,
che organizza le diverse dimensioni in base al problema e al suo effettivo
utilizzo. Si può avere fino a tre dimensioni diverse (x,y,z) sia per
la griglia che per i blocchi. 
Ad ogni blocco, come per ogni threads, è assegnato un indice che può essere
ottenuto tramite altre keywords.   
Le keywords \textsf{threadIdx.x} e \textsf{threadIdx.y} (e in caso anche
\textsf{threadIdx.z}) si riferiscono all'indice dei threads all'interno di un
blocco. Tutti i threads eseguono lo stesso codice presente all'interno di un
kernel, quindi abbiamo bisogno di un meccanismo per distinguerli in modo da
potergli dare direttive diverse o gestire il loro comportamento.
Come per i threads anche i blocchi hanno delle specifiche keywords per risalire
alle loro coordinate. \textsf{blockIdx.x} e \textsf{blockIdx.y} hanno il compito
di ritornare il valore delle coordinate per ogni blocco. Ogni blocco deve avere
lo stesso numero di threads.

Spesso i programmatori CUDA utilizzano la \textsf{struct} \textsf{dim3} per
dichiarare la dimensione di griglie e blocchi. E' una struttura che contiene tre
diversi interi (le tre dimensioni). Ad esempio se dichiarassimo
\textsf{dim3 dimGrid(3,2,2)} vogliamo far intendere al compilatore che la
dimensione della griglia sarà tridimensionale, dove in particolare la
\textsf{x} avrà valore 3, la \textsf{y} 2 e la \textsf{z} 2. Nel caso in cui
invece dichiarassimo \textsf{dim3 dimGrid(3)} il compilatore comprende che
vogliamo solamente utilizzare una dimensione e imposterà la \textsf{y} e la
\textsf{z} ad 1 automaticamente.

Non dimentichiamo perà che le dimensioni di griglie e blocchi vengono definite
lato host e non all'interno dei kernels.

\begin{figure}[tb] 
\centering 
\includegraphics[width=0.5\columnwidth]{Immagini/cuda_grid} 
\caption[Griglie e blocchi tridimensionali in CUDA]{Un esempio di configurazione
di griglie e blocchi tridimensionale in CUDA.\\}
\label{fig:cuda_grid}
\end{figure} 

In ultimo è bene fare la distinzione tra i tre tipi di funzione che possono
essere dichiarate in un programma CUDA. Il primo tipo sono i kernel accompagnati
dalla keyword \textsf{``\_\_global\_\_''}, descritti in questo paragrafo, gli
altri due tipi sono \textsf{``\_\_device\_\_''} e \textsf{``\_\_host\_\_''}.
Come si può intuire una funzione di tipo \textsf{``\_\_device\_\_''} può essere
richiamata dai kernels e dunque verrà lanciata lato device, mentre
\textsf{``\_\_host\_\_''} sarà una funzione che verrà richiamata lato host, in
cui non avviene nessun parallelismo.
Nel caso in cui una funzione viene accompagnata da \textsf{``\_\_host\_\_''} e
\textsf{``\_\_device\_\_''} insieme, il compilatore genera due versioni della
funzione diverse: una per il device e un'altra per l'host.
Se una funzione invece non possiede nessuna keyword, implicitamente verrà
compilata come una funzione host.

Per lanciare un kernel, bisogna aggiungere alla chiamata a funzione la sua
configurazione definita all'interno di $ \langle\langle\langle $ e
$ \rangle\rangle\rangle $.
Al loro interno vanno definiti i parametri relativi alla dimensione di griglie
e blocchi. Un esempio lo troviamo in \ref{lst:esempio_kernel}.

Naturalmente la dimensioni di griglie e blocchi sono limitate in base alla
scheda grafica presente sulla macchina. Ad esempio sulla scheda GTX 680 il
massimo numero di threads per blocchi è 1024 e la dimensione massima di un
blocco è  \begin{math}1024 \times 1024 \times 64\end{math}.

\medskip
\lstinputlisting[caption={Esempio del lancio di un kernel con griglie e
blocchi definiti con la struct dim3.}, label=lst:esempio_kernel,
style=input]{code/esempio_kernel.c}

\subsection{La memoria}

In CUDA, host e device hanno spazi di memoria separati. L'hardware dei devices
sono dotati di random memory access propri (DRAM). Quindi per eseguire un kernel
sul device, il programmatore ha bisogno di allocare la memoria sul device e
trasferire le informazioni pertinenti ai dati sui cui si vuole agire
parallelamente dalla memoria sull'host verso la memoria allocata sul device. Il
sistema CUDA fornisce al programmatore, tramite le sue API, le funzioni per
gestire le allocazioni e i trasferimenti tra le memorie sull'host e sul device.

Le funzioni C \textsf{malloc( \dots)} e \textsf{memcpy( \dots)} sono riproposte
da CUDA C con la versione \textsf{cudaMalloc( \dots)} e \textsf{cudaMemcpy(
\dots)} che eseguono rispettivamente un'allocazione sulla memoria device e una
trasferimento di dati tra la memoria sull'host e la memoria sul device. In
particolare \textsf{cudaMemcpy( \dots)} ha bisogno di ricevere in input anche la
direzione del trasferimento dei dati (da host a device e viceversa). 
Ecco alcuni esempi delle due funzioni citate:

\medskip
\lstinputlisting[caption={Allocazione e trasferimenti dei dati tra CPU e GPU
utilizzando CUDA C.}, label=lst:esempio_mem,
style=input]{code/esempio_mem.c}

Questa è la prima teoria da conoscere ma, come vedremo, ci sono diversi tipi di
memoria a cui un thread può accedere all'interno del device. I tipi di memoria
possono essere classificate per grado di privacy oppure sulla loro velocità.
Tutti i threads possono accedere liberamente alla \textbf{global memory}
chiamata anche comunemente \textit{device memory}. I threads all'interno dello
stesso blocco possono accedere ad una memoria condivisa, chiamata \textbf{shared
memory}, utilizzata per la loro cooperazione, ed infine tutti possiedono una
memoria locale chiamata \textbf{registro}.


\begin{figure}[h] 
\centering 
\includegraphics[width=0.8\columnwidth]{Immagini/memory_model} 
\caption[CUDA Memory]{La struttura della memoria gestita dal sistema
CUDA.\\}
\label{fig:memory_model}
\end{figure} 


Ci sono anche due diversi tipi di spazi di memoria che possono essere utilizzati
dai threads: la memoria costante e la texture memories. Ognuna di loro ha un uso
particolare, ad esempio la constant memory viene utilizzata per salvare i dati
che non cambieranno in tutto il ciclo di vita del kernel.

\subsubsection{La global memory}
\label{par:global_memory}
Lo spazio di memoria più utilizzato per la lettura e la scrittura dei dati è la
global memory, allocata e completamente gestita lato host.
In particolare, in modo da ottimizzare l'accesso alla DRAM non c'è nessun
controllo di consistenza e più threads possono scrivere e leggere allo stesso
tempo senza nessun meccanismo di esclusività. Per questo le varie incoerenze
devono essere completamente gestite dal programmatore.

\subsubsection{La shared memory}
\label{par:shared_memory}
La shared memory è una parte di memoria utilizzata per condividere dati tra
threads all'interno dello stesso blocco. Ogni thread dunque può leggere,
scrivere e modificare dati presenti sulla shared memory ma non può eseguire le
stesse operazioni sulla shared memory di un altro blocco. CUDA offre un ottimo
meccanismo per consentire una comunicazione e cooperazione dei threads
ottimizzata. In particolare una motivazione per cui utilizzare la memoria
condivisa è la differenza di velocità rispetto alla global memory, già con
semplici esempi come la moltiplicazione tra matrici si può notare come
l'utilizzo della shared memory rispetto alla global memory, comporta un
ottimizzazione dell'algoritmo. Un altra differenza rispetto alla global memory è
che la durata di vita della shared memory è uguale al ciclo di un kernel, cioè
al termine del kernel anche la shared memory terminerà il suo lavoro
rilasciando i dati salvati in precedenza.

In particolare la shared memory è suddivisa in banks, in cui ogni bank può
eseguire solo una richiesta per volta. 

\begin{figure}[h] 
\centering 
\includegraphics[width=0.5\columnwidth]{Immagini/banks} 
\caption[Shared memory]{Shared memory divisa in banks.\\}
\label{fig:banks}
\end{figure}

\subsubsection{La constant memory}
\label{par:costant_memory}
Una parte della memoria sul device è la costant memory, che consente di salvare
un limitato numero di simboli, precisamente 64KB. Si può accedere a questo tipo
di memoria solo in modalità lettura. In particolare può essere
utilizzata per aumentare le performance di accesso ai dati che devono essere
condivisi da tutti i threads. La keyword utilizzata per salvare determinati dati
sulla memoria costante è: \textsf{``\_\_constant\_\_''}.

\subsection{Atomicità}
Come scritto in precedenza, la global memory non gestisce nessun tipo di
inconsistenza dei dati. Per questo è il programmatore che deve gestire la
scrittura e la lettura concorrente. Per fare ciò, le API di CUDA, forniscono
diverse funzioni che favoriscono la mutua esclusione per i dati a cui i threads
accedono. In particolare vengono fornite diverse funzioni, tra cui le più note
sono quelle relative alle operazioni aritmetiche. Facciamo un breve elenco delle
funzioni atomiche fornite dalle API di CUDA:

\begin{description}
  \item [atomicAdd()] gestisce l'esclusività per l'operazione somma.
  \item [atomicSub()] gestisce l'esclusività per l'operazione sottrazione.
  \item [atomicMin()] gestisce l'esclusività per il calcolo del minimo.
  \item [atomicMax()] gestisce l'esclusività per il calcolo del massimo.
  \item [atomicInc()] gestisce l'esclusività per l'operazione di incremento.
  \item [atomicDec()] gestisce l'esclusività per l'operazione di decremento.
  \item [atomicAnd()] gestisce l'esclusività per l'operazione \textit{AND}.
  \item [atomicOr()] gestisce l'esclusività per l'operazione \textit{OR}.
  \item [atomicXor()] gestisce l'esclusività per l'operazione \textit{XOR}.
  \item [atomicCAS()] gestisce l'esclusività per l'operazione di
  \textit{compare and swap}.
\end{description}

Grazie a queste funzioni, un programmatore CUDA può gestire le concorrenze
quando c'è strettamente bisogno della mutua esclusione.

\subsection{Parallelismo dinamico}
\label{par:Parallelismo_dinamico}
Il parallelismo dinamico è un estensione di CUDA, introdotta con CUDA 5.0, che
consente la creazione e la sincronizzazione di un kernel direttamente dal
device. Sfruttare questa opportunità comporta diversi vantaggi in termini di
performance, infatti creare \textit{lavoro} direttamente da GPU può ridurre il
bisogno di trasferire dati tra host e device come il controllo dell'esecuzione e
sincronizzazione. In particolare questa nuova feature consente al programmatore
di gestire la configurazione dei threads anche a runtime direttamente dal
device. Stessa opportunità si ha per il parallelismo dei dati che può essere
generato direttamente all'interno di un kernel prendendo i vantaggi che
l'hardware della GPU offre come lo scheduling e il load balancing.

Il parallelismo dinamico è supportato dai device con una compute capability pari
a 3.5 o superiore. \cite{CUDA:2012}

All'interno di un kernel, un thread può configurare e lanciare una nuova griglia
di blocchi chiamata ``child grid'' mentre la griglia a cui appartiene il
thread si chiamerà ``parent grid''. La sincronizzazione tra parent e grid è
implicita nel caso in cui non viene espressamente definita.
L'immagine \ref{fig:dynamic} è un chiaro esempio di approccio al parallelismo
dinamico.

\begin{figure}[H] 
\centering 
\includegraphics[width=0.6\columnwidth]{Immagini/dynamic} 
\caption[Dynamic Parallelism]{Dynamic Parallelism.\\}
\label{fig:dynamic}
\end{figure}

Le griglie parent and grid condividono la stessa memoria globale e la stessa
memoria costante  ma non la shared memory e la memoria locale
(\ref{par:costant_memory}). La coerenza e la consistenza possono diventare un
problema nell'utilizzo del parallelismo dinamico, ragione per cui a volte è
espressamente indicato l'utilizzo di una sincronizzazione esplicita. In generale
ci sono due punti di esecuzione in cui c'è la sicurezza di avere dei dati
consistenti: quando un thread invoca una nuova child grid e quando la child grid
ha completato la sua esecuzione. Comunque sia, la sincronizzazione può avvenire
in qualsiasi momento tramite due funzioni appartenenti alle API di CUDA:
\textsf{cudaDeviceSynchronize()} e \textsf{\_\_syncthreads()}.

\medskip
\lstinputlisting[caption={Esempio di un programma CUDA utilizzando il Dynamic
parallelism.}, label=lst:dynamic, style=input]{code/dynamic.c}

\section{Tools di sviluppo}
Nsight Visual Studio e Nsight Eclipse Edition sono due ottime soluzioni per
implementare un programma CUDA. La distinzione tra i due fondamentale è il
sistema operativo in cui operano: il primo sul sistema Windows e il secondo sui
sistemi Linux e MacOS. 

Spesso, durante le fasi implementative di un programma parallelo, il
programmatore ha bisogno di funzionalità per ottimizzare i tempi e le
performance di un programma. Anche nelle applicazioni sequenziali ormai il Debug
è diventato fondamentale per la corretta implementazione di un programma. In
CUDA, come nel resto dei paradigmi per il parallelismo, non è scontato avere
queste utilità nei software per lo sviluppo. 

Fortunatamente, le soluzioni implementate per CUDA offrono al programmatore
diverse features e tools per ottimizzare il codice e favorire la riuscita di una
buona implementazione. 
Nei sistemi Linux e MAC troviamo \textbf{CUDA-GDB}), tool
di NVIDIA, che consente il debugging delle applicazioni CUDA. 
Un altro tool degno di menzione è \textbf{CUDA-MEMCHECK}, incluso in
CUDA Toolkit, che controlla l'accesso alla memoria e i vari errori che possono
essere incontrati in corso di esecuzione (es. out of bounds, errori di accesso
alla memoria etc.).

Gli ambienti Nsight per lo sviluppo di applicazioni offrono un sistema user
friendly che facilità la compilazione dellle applicazioni CUDA. Visual Profiler
invece risulta essere di vitale importanza ai fini della performance consentendo ai
programmatori di comprendere e ottimizzare le applicazioni CUDA. La potenza del
profiler è la facile comprensione del risultato, molto simile ad un diagramma di
Gantt, che mostra a video le attività della CPU e della GPU includendo analisi
automatiche sull'applicazione identificando opportunità di miglioramento della
performance.

\subsection{Nsight Visual Studio}
Visual Studio è un ambiente di sviluppo molto conosciuto dai programmatori.
E' sviluppato da Microsoft e supporta diversi linguaggi di programmazione quali
C, C++, C\#, ASP .Net. Inoltre è un ambiente di sviluppo multipiattaforma con
cui poter realizzare applicazioni per PC, Server ma anche web applications e
applicazioni per smartphone.

Nel suo più comune utilizzo offre in dotazione un debugger e un compilatore per
il linguaggi citati.

La versione Nsight è utilizzata dagli sviluppatori CUDA e fornisce diversi
strumenti per il Debug, il Profiler e la computazione eterogenea per
applicazioni CUDA C/C++.

La sua installazione è semplice e la creazione di progetti è guidata per ogni
tipo di esigenze. In ambiente Windows è veramente immediata l'installazione del
toolkit fornito da NVIDIA, che consente di creare progetti NVIDIA CUDA
direttamente da Visual Studio.

\begin{figure}[H] 
\centering 
\includegraphics[width=1.0\columnwidth]{Immagini/newprojVS} 
\caption[CUDA su Visual Project]{Creazione di un progetto CUDA 6.5 su Visual
Studio.\\}
\label{fig:newprojVS}
\end{figure}

%\subsection{Nsight Eclipse Edition}


\subsection{Visual Profiler}
Il Visual Profiler è un software secondario fornito da NVIDIA utile per
un'analisi approfondita dell'utilizzo della memoria e delle performance in
generale della GPU. E' un ambiente ricco di funzionalità e informazioni utili
che il programmatore può utilizzare ai fini di migliorare il programma CUDA e
migliorarne le prestazioni.

Il software si presenta come in figura \ref{fig:profiler}.

\begin{figure}[H] 
\centering 
\includegraphics[width=1.0\columnwidth]{Immagini/profiler} 
\caption[Visual Profiler]{Esempio di progetto analizzato su VIsual Profiler.\\}
\label{fig:profiler}
\end{figure}

Tra le tante analisi effettuate dal software, quelle che risultano più
interessanti sono sicuramente le informazioni relative al trasferimento di dati
tra GPU e CPU e le informazioni sui tempi impiegati dai kernel e dal loro
effettivo utilizzo.

Sul trasferimento dei dati tra memoria è interessante conoscere anche la
velocità di trasferimento che naturalmente cambia da scheda a scheda e da tipo
di trasferimento. Il trasferimento dei dati più veloce avviene all'interno del
device. Infatti una copia di memoria da device a device, su una scheda video
NVIDIA GT-750M, può arrivare fino a 4,5 TB/s, con trasferimenti che impiegano
nanosecondi. 

Il profiler risulta molto utile in fase di programmazione poiché rende facile
l'individuazione dei kernel ``lenti''. Spesso si abusa di chiamate ai kernel
senza accorgersene e senza profiler è sicuramente più difficile individuare i
punti critici del programma.

Nel lavoro di tesi è stato utilizzato il profiler parecchie volte in fase di
programmazione proprio per implementare la versione più performante della
libreria OpenCAL.
Per poter utilizzare Visual Profiler bisogna creare un nuovo progetto che prende
in input l'eseguibile del progetto CUDA compilato e la cartella dei dati che
vengono utilizzati dal programma. E' anche possibile utilizzare altre
funzionalità valide per le analisi ma possono essere attivate in fase di
profiling. Naturalmente il programmatore potrebbe anche desiderare di analizzare
solo parte del programma, per questo il profiler prende in considerazione
solamente il codice racchiuso tra le chiamate a funzione
\textsf{cudaStartProfiler()} e \textsf{cudaStopProfiler()}. 

Il Visual Profiler è scaricabile facilemente dal sito di NVIDIA, e può essere
utilizzato sia in ambienti Linux/Unix e MacOS che su ambienti Windows.
