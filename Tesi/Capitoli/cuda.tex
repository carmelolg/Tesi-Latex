% !TEX encoding = UTF-8
% !TEX TS-program = pdflatex
% !TEX root = ../Tesi.tex
% !TEX spellcheck = it-IT

%************************************************
\chapter{CUDA - Compute Unified Device Architecture}
\label{cap:CUDA}
%************************************************

\section{Introduzione}
Quasi nove anni fa, nel Novembre 2006  la \textbf{NVIDIA Corporation} ha
rilasciato CUDA, una piattaforma (hardware e software insieme) che permettono di
utilizzare linguaggi di programmazione ad alto livello (Ad es. \textbf{C},
\textbf{C++}, \textbf{Java}) per implementare codice parallelo per risolvere
problemi molto complessi a livello computazionale in una maniera efficiente
rispetto alle normali CPU.

\begin{figure}[h] 
\centering 
\includegraphics[width=0.5\columnwidth]{Immagini/cuda_compiler} 
\caption[Compilatore NVCC]{Struttura di Nvidia C Compiler.\\}
\label{fig:cuda_compiler} 
\end{figure}

CUDA è molto utilizzato poiché è un sistema completo e anche molto semplice da
capire ed utilizzare. Sopratutto quest'ultimo particolare è di importante
rilevanza, dato che attualmente le alternative a CUDA, come OpenCL, risultano
essere molto più complesse a livello implementativo e di leggibilità  del
codice.
Come illustrato in figura \ref{fig:cuda_compiler}, NVIDIA fornisce un
compilatore capace di riconoscere le istruzioni CUDAma l'implementazione di un
programma parallelo avviene utilizzando codice sorgente sia per CPU che per GPU.
Il compilatore NVIDIA C (\textit{nvcc}) dunque, identifica il tipo di istruzione
richiamando i compilatori di riferimento gestendo così questa convivenza.
  
\section{Architettura hardware}

Oggi sul mercato delle schede video possiamo trovare innumerevoli tipi di
device, e i computer moderni referibilmente posseggono una scheda video
dedicata. In particolare la \textbf{Nvidia Corporation} ha creato anche diverse
architetture hardware per soddisfare ogni tipo di richiesta. Quelle conosciute
sono le architetture \textbf{Kepler}, \textbf{Fermi} e \textbf{Tesla}.
L'architettura Kepler è quella più utilizzata nei computer in commercio con
scheda grafica NVIDIA.

 In generale, le architetture GPU NVIDIA, sono composte da un array di
 \textit{Streaming Multiprocessors (SMs)}. Lo Streaming Multiprocessors è
 progettato per eseguire centinaia di threads in parallelo e contiene un
 determinato numero di Streaming Processors (SP). Gli Streaming processors sono
 anche chiamati \textit{CUDA cores} e il loro numero dipende dalla capacità  del
 device installato.
 
 \subsection{Compute capability}
Ogni device possiede un \textit{revision number} che possiamo definire come la
\textbf{compute capability} del device, e determina l'insieme di funzionalità 
che possono essere usate nell'implementazione di codice parallelo in CUDA.
La compute capability è definita dal più alto numero di revision number e il
minor numero di revision number. Nel caso in cui devices diversi abbiano il più
alto revision number posseggono la stessa architettura. Il più alto numero di
revision number per le architetture Kepler è 3, per i devices basati su
un'architettura Fermi è 2, mentre per i device con architettura Tesla 1. Il
numero minore di revision number invece, corrisponde al miglioramento del core
dell'architettura che spesso porta a nuove funzionalità  da poter utilizzare
tramite le API fornite appunto da NVIDIA.

\subsection{Architettura Kepler}
L'architettura Kepler è stata progettata e successivamente lanciata nel 2010
insieme all'architettura Fermi. La prima GPU basata sull'architettura Kepler si
chiamava ``GK104" in cui ogni unità  interna è stata progettata ai fini di
avere la miglior performance per watt (perf/watt). Alcuni esperti hanno
affermato che la GK104 Kepler è la GPU più potente per la computazione e il
rendering grafico dei videogames.

Inizialmente la GPU utilizzata per questo lavoro di tesi è stata la NVIDIA
GeForce GT 750M basata anch'essa su un architettura Kepler. Il core in
particolare è il ``GK107" che offre due shader di blocchi, chiamati
\textbf{SMX}, ognuno dei quali ha 192 shaders per un totale di 384 shader cores
con una velocità  di 967 MHz.

\begin{figure}[h] 
\centering 
\includegraphics[width=0.5\columnwidth]{Immagini/gt750m} 
\caption[GT 750M]{La scheda video NVIDIA GT 750-M.\\}
\label{fig:gt750m} 
\end{figure} 

\section{Interfaccia di programmazione}
Un programma CUDA consiste in una o più fasi che sono eseguite sia lato host
(\textbf{CPU}) che lato device (\textbf{GPU}). Le fasi in cui l'ammontare
computazionale non è eccessivo, e dunque non siamo in presenza di
parallelismo dei dati, vengono implementate lato host, mentre le fasi che
richiedono un grosso ammontare di parallelismo dei dati sono implementate lato
device. CUDA consente di creare un unico file sorgente con codice host e device
insieme. Il compilatore NVIDIA C (\textbf{nvcc} fig. \ref{fig:cuda_compiler})
separa le due diverse implementazioni durante il processo di compilazione.

Il linguaggio per scrivere codice sorgente lato device è ANSI C, esteso con
particolari \textit{keywords} per far comprendere al compilatore quali sono le
funzioni con la presenza di parallelismo. Queste funzioni sono chiamate
\textbf{\textit{kernels}}. Per utilizzare nvcc naturalmente dobbiamo essere in
possesso di una GPU NVidia correttamente montata sulla propria macchina, ma se
così non fosse si può emulare su CPU le features di CUDA per poter eseguire i
kernels (MCUDA tool etc.).

Le funzioni kernel generano un determinato di threads eseguiti in parallelo per
raggiungere il data parallelism. Ad esempio per la somma di due matrici può
essere implementata come un kernel dove ogni threads computa un elemento
dell'output. Il massimo del parallelismo si ha quando ad ogni threads è
associata una cella della matrice. Se la dimensione della matrice è 1000 x 1000
servono 1 milione di threads per raggiungere il nostro scopo. Lato CPU per
generare e eseguire lo scheduling di un enorme numero di threads è
particolarmente oneroso, mentre in CUDA c'è un ottimo supporto hardware da
questo punto di vista, dunque il programmatore può sorvolare su questo tipo di
problema.

\begin{figure}[h] 
\centering 
\includegraphics[width=0.5\columnwidth]{Immagini/cuda_program} 
\caption[GT 750M]{Esecuzione di un programma CUDA.\\}
\label{fig:cuda_program} 
\end{figure} 

Una tipica esecuzione di un programma CUDA è mostrata nella Fig.
\ref{fig:cuda_program}.
L'esecuzione viene eseguita a strati, la prima ad essere eseguita è la parte
host (CPU) per poi susseguirsi un insieme di strati che possono comportare anche
il lancio dei kernels nel caso ci siano parti parallelizzate. I threads sono
inglobati all'interno di \textbf{blocchi} che a loro volta sono parte di una
griglia di blocchi chiamata \textbf{grid}. Quando un kernel termina, il
programma continua con l'esecuzione lato host fino a che un nuovo kernel viene
lanciato.

\subsection{I kernel}

Come detto in precedenza, la funzione \textit{kernel} specifica il codice che
deve essere eseguito da tutti i threads lanciati nella fase parallela di un
programma CUDA. Tutti i threads lanciati in parallelo eseguono lo stesso
codice, infatti un programma CUDA non è nient'altro che l'applicazione pratica
del modello Single-Program Multiple-Data (Tassonomia di Flynn \ref{par:flynn}).
Questa tecnica è molto utilizzata nei sistemi paralleli. 

Per poter dichiarare un kernel c'è una specifica keyword di CUDA da utilizzare:
``\textbf{\_\_global\_\_}''. Questa keyword indica che la funzione è un kernel e
questa funzione richiamata dall'host genererà una griglia di threads sul device,
in particolare può solamente essere richiamata lato host (a
meno che non ci sia un ambiente addatto per potere utilizzare il parallelismo dinamico
\ref{par:Parallelismo_dinamico}).
CUDA genera threads suddivisi in blocchi, ed ogni blocco appartiene ad una
griglia. Lo schema è mostrato in figura \ref{fig:grid_block}.

\begin{figure}[h] 
\centering 
\includegraphics[width=0.5\columnwidth]{Immagini/grid_block} 
\caption[Griglie e blocchi cuda]{Esempio generico di griglie e blocchi in un
programma CUDA.\\}
\label{fig:grid_block}
\end{figure} 

In realtà, la dimensione della griglia e dei blocchi la decide il programmatore,
che organizza le diverse dimensioni in base al problema e al suo effettivo
utilizzo. Si può avere fino a tre dimensioni diverse (x,y,z) sia per
la griglia che per i blocchi. 
Ad ogni blocco, come per ogni threads, è assegnato un indice che può essere
ottenuto tramite altre keywords.   
Le keywords \textsf{threadIdx.x} e \textsf{threadIdx.y} (e in caso anche
\textsf{threadIdx.z}) si riferiscono all'indice dei threads all'interno di un
blocco. Tutti i threads eseguono lo stesso codice presente all'interno di un
kernel, quindi abbiamo bisogno di un meccanismo per distinguerli in modo da
potergli dare direttive diverse o gestire il loro comportamento.
Come per i threads anche i blocchi hanno delle specifiche keywords per risalire
alle loro coordinate. \textsf{blockIdx.x} e \textsf{blockIdx.y} hanno il compito
di ritornare il valore delle coordinate per ogni blocco. Ogni blocco deve avere
lo stesso numero di threads.

Spesso i programmatori CUDA utilizzano la \textsf{struct} \textsf{dim3} per
dichiarare la dimensione di griglie e blocchi. E' una struttura che contiene tre
diversi interi (le tre dimensioni). Ad esempio se dichiarassimo
\textsf{dim3 dimGrid(3,2,2)} vogliamo far intendere al compilatore che la
dimensione della griglia sarà tridimensionale, dove in particolare la
\textsf{x} avrà valore 3, la \textsf{y} 2 e la \textsf{z} 2. Nel caso in cui
invece dichiarassimo \textsf{dim3 dimGrid(3)} il compilatore comprende che
vogliamo solamente utilizzare una dimensione e imposterà la \textsf{y} e la
\textsf{z} ad 1 automaticamente.

Non dimentichiamo perà che le dimensioni di griglie e blocchi vengono definite
lato host e non all'interno dei kernels.

\begin{figure}[tb] 
\centering 
\includegraphics[width=0.5\columnwidth]{Immagini/cuda_grid} 
\caption[Griglie e blocchi cuda]{Un esempio di configurazione di griglie e
blocchi tridimensionale in CUDA.\\}
\label{fig:cuda_grid}
\end{figure} 

In ultimo è bene fare la distinzione tra i tre tipi di funzione che possono
essere dichiarate in un programma CUDA. Il primo tipo sono i kernel accompagnati
dalla keyword \textsf{``\_\_global\_\_''}, descritti in questo paragrafo, gli
altri due tipi sono \textsf{``\_\_device\_\_''} e \textsf{``\_\_host\_\_''}.
Come si può intuire una funzione di tipo \textsf{``\_\_device\_\_''} può essere
richiamata dai kernels e dunque verrà lanciata lato device, mentre
\textsf{``\_\_host\_\_''} sarà una funzione che verrà richiamata lato host, in
cui non avviene nessun parallelismo.
Nel caso in cui una funzione viene accompagnata da \textsf{``\_\_host\_\_''} e
\textsf{``\_\_device\_\_''} insieme, il compilatore genera due versioni della
funzione diverse: una per il device e un'altra per l'host.
Se una funzione invece non possiede nessuna keyword, implicitamente verrà
compilata come una funzione host.

Per lanciare un kernel, bisogna aggiungere alla chiamata a funzione la sua
configurazione definita all'interno di $ \langle\langle\langle $ e
$ \rangle\rangle\rangle $.
Al loro interno vanno definiti i parametri relativi alla dimensione di griglie
e blocchi. Un esempio lo troviamo in \ref{lst:esempio_kernel}.

Naturalmente la dimensioni di griglie e blocchi sono limitate in base alla
scheda grafica presente sulla macchina. Ad esempio sulla scheda GTX 680 il
massimo numero di threads per blocchi è 1024 e la dimensione massima di un
blocco è  \begin{math}1024 \times 1024 \times 64\end{math}.

\medskip
\lstinputlisting[caption={Esempio del lancio di un kernel con griglie e
blocchi definiti con la struct dim3.}, label=lst:esempio_kernel,
style=input]{code/esempio_kernel.c}

\subsection{La memoria}

In CUDA, host e device hanno spazi di memoria separati. L'hardware dei devices
sono dotati di random memory access propri (DRAM). Quindi per eseguire un kernel
sul device, il programmatore ha bisogno di allocare la memoria sul device e
trasferire le informazioni pertinenti ai dati sui cui si vuole agire
parallelamente dalla memoria sull'host verso la memoria allocata sul device. Il
sistema CUDA fornisce al programmatore, tramite le sue API, le funzioni per
gestire le allocazioni e i trasferimenti tra le memorie sull'host e sul device.

Le funzioni C \textsf{malloc( \dots)} e \textsf{memcpy( \dots)} sono riproposte
da CUDA C con la versione \textsf{cudaMalloc( \dots)} e \textsf{cudaMemcpy(
\dots)} che eseguono rispettivamente un'allocazione sulla memoria device e una
trasferimento di dati tra la memoria sull'host e la memoria sul device. In
particolare \textsf{cudaMemcpy( \dots)} ha bisogno di ricevere in input anche la
direzione del trasferimento dei dati (da host a device e viceversa). 
Ecco alcuni esempi delle due funzioni citate:

\medskip
\lstinputlisting[caption={Allocazione e trasferimenti dei dati tra CPU e GPU
utilizzando CUDA C.}, label=lst:esempio_mem,
style=input]{code/esempio_mem.c}

Questa è la prima teoria da conoscere ma, come vedremo, ci sono diversi tipi di
memoria a cui un thread può accedere all'interno del device. I tipi di memoria
possono essere classificate per grado di privacy oppure sulla loro velocità.
Tutti i threads possono accedere liberamente alla \textbf{global memory}
chiamata anche comunemente \textit{device memory}. I threads all'interno dello
stesso blocco possono accedere ad una memoria condivisa, chiamata \textbf{shared
memory}, utilizzata per la loro cooperazione, ed infine tutti possiedono una
memoria locale chiamata \textbf{registro}.


\begin{figure}[h] 
\centering 
\includegraphics[width=0.8\columnwidth]{Immagini/memory_model} 
\caption[Memoria in CUDA]{La struttura della memoria gestita dal sistema
CUDA.\\}
\label{fig:memory_model}
\end{figure} 


Ci sono anche due diversi tipi di spazi di memoria che possono essere utilizzati
dai threads: la memoria costante e la texture memories. Ognuna di loro ha un uso
particolare, ad esempio la constant memory viene utilizzata per salvare i dati
che non cambieranno in tutto il ciclo di vita del kernel.

\subsubsection{La global memory}
Lo spazio di memoria più utilizzato per la lettura e la scrittura dei dati è la
global memory, allocata e completamente gestita lato host.
In particolare, in modo da ottimizzare l'accesso alla DRAM non c'è nessun
controllo di consistenza e più threads possono scrivere e leggere allo stesso
tempo senza nessun meccanismo di esclusività. Per questo le varie incoerenze
devono essere completamente gestite dal programmatore.

\subsubsection{La shared memory}
La shared memory è una parte di memoria utilizzata per condividere dati tra
threads all'interno dello stesso blocco. Ogni thread dunque può leggere,
scrivere e modificare dati presenti sulla shared memory ma non può eseguire le
stesse operazioni sulla shared memory di un altro blocco. CUDA offre un ottimo
meccanismo per consentire una comunicazione e cooperazione dei threads
ottimizzata. In particolare una motivazione per cui utilizzare la memoria
condivisa è la differenza di velocità rispetto alla global memory, già con
semplici esempi come la moltiplicazione tra matrici si può notare come
l'utilizzo della shared memory rispetto alla global memory, comporta un
ottimizzazione dell'algoritmo. Un altra differenza rispetto alla global memory è
che la durata di vita della shared memory è uguale al ciclo di un kernel, cioè
al termine del kernel anche la shared memory terminerà il suo lavoro
rilasciando i dati salvati in precedenza.

In particolare la shared memory è suddivisa in banks, in cui ogni bank può
eseguire solo una richiesta per volta. 

\begin{figure}[h] 
\centering 
\includegraphics[width=0.5\columnwidth]{Immagini/banks} 
\caption[Memoria in CUDA]{Shared memory divisa in banks.\\}
\label{fig:banks}
\end{figure}

\subsubsection{La constant memory}
Una parte della memoria sul device è la costant memory, che consente di salvare
un limitato numero di simboli, precisamente 64KB. Si può accedere a questo tipo
di memoria solo in modalità lettura. In particolare può essere
utilizzata per aumentare le performance di accesso ai dati che devono essere
condivisi da tutti i threads. La keyword utilizzata per salvare determinati dati
sulla memoria costante è: \textsf{``\_\_constant\_\_''}.

\subsection{Atomicità}
Come scritto in precedenza, la global memory non gestisce nessun tipo di
inconsistenza dei dati. Per questo è il programmatore che deve gestire la
scrittura e la lettura concorrente. Per fare ciò, le API di CUDA, forniscono
diverse funzioni che favoriscono la mutua esclusione per i dati a cui i threads
accedono. In particolare vengono fornite diverse funzioni, tra cui le più note
sono quelle relative alle operazioni aritmetiche. Facciamo un breve elenco delle
funzioni atomiche fornite dalle API di CUDA:

\begin{description}
  \item [atomicAdd()] gestisce l'esclusività per l'operazione somma.
  \item [atomicSub()] gestisce l'esclusività per l'operazione sottrazione.
  \item [atomicMin()] gestisce l'esclusività per il calcolo del minimo.
  \item [atomicMax()] gestisce l'esclusività per il calcolo del massimo.
  \item [atomicInc()] gestisce l'esclusività per l'operazione di incremento.
  \item [atomicDec()] gestisce l'esclusività per l'operazione di decremento.
  \item [atomicAnd()] gestisce l'esclusività per l'operazione \textit{AND}.
  \item [atomicOr()] gestisce l'esclusività per l'operazione \textit{OR}.
  \item [atomicXor()] gestisce l'esclusività per l'operazione \textit{XOR}.
  \item [atomicCAS()] gestisce l'esclusività per l'operazione di
  \textit{compare and swap}.
\end{description}

Grazie a queste funzioni, un programmatore CUDA può gestire le concorrenze
quando c'è strettamente bisogno della mutua esclusione.

\subsection{Parallelismo dinamico}
\label{par:Parallelismo_dinamico}

\section{Tools di sviluppo}
\subsection{Nsight}
\subsection{Visual Profiler}